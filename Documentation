Documentation:

Accuracy for using Naive Bayes and Decision trees when the features are every word up to the first laugh: ~55% so might as well be guessing

Accuracy for using Naive Bayes and Decision trees with only the last paragraph before the laugh, using all the words in the paragraph as features: ~77% This is significantly more than just using the whole text.

When using naive bayes with the ngrams (2-4grams for characters and words), the results are REALLY low. ~20%
While using Decision tree is still pretty high, about 78%

After testing the percentage of nouns, verbs, and adjectives 3 times, I got the following data. 
mean Noun:  0.20232220197213233
mean Noun:  0.20001447637842718
mean Noun:  0.20115009755835697

std_dev Noun:  0.060288939529409565
std_dev Noun:  0.05825488402818412
std_dev Noun:  0.05860120254432491

mean Adjective:  0.061696352526332654
mean Adjective:  0.06274923028735785
mean Adjective:  0.06250457723806231

std_dev Adjective:  0.035619442016751414
std_dev Adjective:  0.0343900788699468
std_dev Adjective:  0.03392326607613692

mean Verb:  0.1732463822749182
mean Verb:  0.17348569290550092
mean Verb:  0.1730733211025457

std_dev Verb:  0.04681431031330794
std_dev Verb:  0.045695402129826916
std_dev Verb:  0.04415720204037863

I set the bin cutoffs to:
nouns     : low <.145 medium < .255
adjectives: low <.028 medium < .096
verbs     : low <.13 medium < .22 

This is important for binning the adjectives by percentage for naive bayes and decision tree
The results of using only the percentages was ~74% for both decision tree and naive bayes

Running Maximum Entropy with words alone had the following results: 78.23% accuracy but took 1.25 hours to run
Running Maximum Entropy with ngrams had the following results: 21.26% accuracy and didn't take long
Running Maximum Entropy with POS had the following results: 78.74% accuracy but took 1.25 hours to run
Running Maximum Entropy with all 3 (once) resulted in 86.7% accuracy. It took a long time to run however.

When using both words and POS tagging, the results were:
Decision Tree: 71%
Naive Bayes: 46.88%

When using both words and ngrams, the results were:
Decision Tree: 80%
Naive Bayes: 78%
However, there was massive variation. On a few occaisions, the results would suddenly drop to about 23% accuracy. And just as often it would shoot up to about 84%.

When using Ngrams and POS tagging, the results varied greatly. Below are the results:
Naive   DTree
.42    .79
.728   .824
.322   .79
.642   .206
.787   .843

When using all of the features, the Naive bayes results varied. The decision tree results had a sudden drop, but was much more consistent. Below are the results:
Naive   DTree
.748   .834
.468   .758
.699   .179
.758   .782
.74    .827


After switching the data to being each individual paragraph, the accuracy plummeted to random level. Paragraphs were marked positive based on if they had a laugh in them or immedietly after them. This created a lot of data, which actually took a very long time to run. We also added a sentiment analyzer with textblob that didn't increase the accuracy. It analyzed the whole paragraph however, and I believe it would work better only analyzing the last sentence or 2. The idea comes from a paper saying that humor is usually negative. They found evidence of this using a sentiment analyzer. They also found that personal pronouns are a strong sign of humor as well since humor is human focused.

Anyway, we are now going to look at different data. Instead of looking at each paragraph, we are looking at what has already been said as well. Now, positive data is everything up to a laugh. This can mean there were previous laughs before. This also applies for negative data, which is data that goes up to a similar length but doesn't end with a laugh. There may have been laughs before in negative data as well. 

The previous laughs actually will be a new feature. How many there were before, how deep the paragraph is in the text, how long it has been since another laugh, etc. All of which play a role in the features now. We are going to get into some crazy stuff now. To think, I thought I was getting close with the high level of accuracy described above. Well, let's do this! - Start of winter quarter

Look at FeatureData_Jan_18th for the current data on the features. All of those were done with 2000 pieces of data (1k positive and 1k negative) at least. I need to convert to using sci-kit learn classifiers though. They are supposed to be more powerful as it is designed for this purpose, unlike NLTK.

For the Word2Vec, "word2vec/text8" is the text file. "word2vec/text8.bin" is the binary for a stored word2vec model. all of the "text8.model" stuff is used for another stored word2vec model that can be trained more! Unfortunately, right now the word2vec model has a hard time with certain words such as professions (mason, plumber, etc), uncommon fruits (papaya), and numbers in general. As well as uncommon words like "biogas" and "rawest". Because of that, I am storing the word vec in the feature collection so that I can try better trained models on it, as well as play with the feature itself.

So for the incongruity, there are two methods. One where they check the for the highest/lowest distance between each pair of words in the whole sentences (so [1,2], [1,3], [1,4], [2,3], etc). And the other only checks pairs of words (so [1,2], [2,3], [3,4], etc). The full one works better with the other features, and the other works best by itself and without the cheating knowledge. Combined, it is worse than the full when the cheat data is included, but better than pairs. And vice versa for when the laugh knowledge is excluded. I'm going to check to see if the laugh data is included and if so do full, otherwise do pairs. This is all done on paragraph and not sentences.

When I tried to use this with sentences, the results were much less impressive. The f1 score improved by around half a percent. Which is decent but still not that good. When done by itself, the results was around 52% accurate, which is not much higher than the 50% baseline. Therefore, this is is significantly more useful for the by paragraph results. 

Note that all of the data on incongruity is based on word2vec being trained on text8, which is pretty weak. I attempted to download a stronger one, but I ran out of memory.

You can download word2vec prebuilt here for the big one
https://docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download
and here for the small one
http://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb
or here
https://github.com/idio/wiki2vec/
or
https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models

The next thing I tried to do is remove the unimportant features using selectKBest. The results for paragraphs were that reducing the features helped, but, at best, it was just as good as when it wasn't there. 

I have added swearing recognition, and having an exclamation mark has a feature. The results are still coming in, but it doesn't seem to have helped the Naive Bayes Classifier for sentences. 

I tried putting in entity recognition roughly, but it made it worse since it wasn't perfect :/

I have put in detecting when there is a statistic (roughly) and how many

I have also tried to measure complexity by parsing it with the stanford parser and seeing how many parens there are and how many subtrees there are. It didn't do anything...

BUT! Good news, I used frequency data to look for rare words! That turned out to be helpful. I am also in the process of checking for words that happen only once!

As of August 3rd:
Best with sentences without laughter feats is naive bayes
Best with sentences with laughter feats is adaboost
Best with paras without laughter feats is naive bayes
Best with paras with laughter feats is adaboost