Documentation:

Accuracy for using Naive Bayes and Decision trees when the features are every word up to the first laugh: ~55% so might as well be guessing

Accuracy for using Naive Bayes and Decision trees with only the last paragraph before the laugh, using all the words in the paragraph as features: ~77% This is significantly more than just using the whole text.

When using naive bayes with the ngrams (2-4grams for characters and words), the results are REALLY low. ~20%
While using Decision tree is still pretty high, about 78%

After testing the percentage of nouns, verbs, and adjectives 3 times, I got the following data. 
mean Noun:  0.20232220197213233
mean Noun:  0.20001447637842718
mean Noun:  0.20115009755835697

std_dev Noun:  0.060288939529409565
std_dev Noun:  0.05825488402818412
std_dev Noun:  0.05860120254432491

mean Adjective:  0.061696352526332654
mean Adjective:  0.06274923028735785
mean Adjective:  0.06250457723806231

std_dev Adjective:  0.035619442016751414
std_dev Adjective:  0.0343900788699468
std_dev Adjective:  0.03392326607613692

mean Verb:  0.1732463822749182
mean Verb:  0.17348569290550092
mean Verb:  0.1730733211025457

std_dev Verb:  0.04681431031330794
std_dev Verb:  0.045695402129826916
std_dev Verb:  0.04415720204037863

I set the bin cutoffs to:
nouns     : low <.145 medium < .255
adjectives: low <.028 medium < .096
verbs     : low <.13 medium < .22 

This is important for binning the adjectives by percentage for naive bayes and decision tree
The results of using only the percentages was ~74% for both decision tree and naive bayes

Running Maximum Entropy with words alone had the following results: 78.23% accuracy but took 1.25 hours to run
Running Maximum Entropy with ngrams had the following results: 21.26% accuracy and didn't take long
Running Maximum Entropy with POS had the following results: 78.74% accuracy but took 1.25 hours to run
Running Maximum Entropy with all 3 (once) resulted in 86.7% accuracy. It took a long time to run however.

When using both words and POS tagging, the results were:
Decision Tree: 71%
Naive Bayes: 46.88%

When using both words and ngrams, the results were:
Decision Tree: 80%
Naive Bayes: 78%
However, there was massive variation. On a few occaisions, the results would suddenly drop to about 23% accuracy. And just as often it would shoot up to about 84%.

When using Ngrams and POS tagging, the results varied greatly. Below are the results:
Naive   DTree
.42    .79
.728   .824
.322   .79
.642   .206
.787   .843

When using all of the features, the Naive bayes results varied. The decision tree results had a sudden drop, but was much more consistent. Below are the results:
Naive   DTree
.748   .834
.468   .758
.699   .179
.758   .782
.74    .827


After switching the data to being each individual paragraph, the accuracy plummeted to random level. Paragraphs were marked positive based on if they had a laugh in them or immedietly after them. This created a lot of data, which actually took a very long time to run. We also added a sentiment analyzer with textblob that didn't increase the accuracy. It analyzed the whole paragraph however, and I believe it would work better only analyzing the last sentence or 2. The idea comes from a paper saying that humor is usually negative. They found evidence of this using a sentiment analyzer. They also found that personal pronouns are a strong sign of humor as well since humor is human focused.

Anyway, we are now going to look at different data. Instead of looking at each paragraph, we are looking at what has already been said as well. Now, positive data is everything up to a laugh. This can mean there were previous laughs before. This also applies for negative data, which is data that goes up to a similar length but doesn't end with a laugh. There may have been laughs before in negative data as well. 

The previous laughs actually will be a new feature. How many there were before, how deep the paragraph is in the text, how long it has been since another laugh, etc. All of which play a role in the features now. We are going to get into some crazy stuff now. To think, I thought I was getting close with the high level of accuracy described above. Well, let's do this! - Start of winter quarter